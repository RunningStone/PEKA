
# file structure:read the configs in the specified path
import sys
import os
notebook_path = os.getcwd()
print(notebook_path)
root = os.path.dirname(os.path.dirname(os.path.dirname(notebook_path)))
print(f"â­ï¸ root folder for project: {root}")
code_root = root + "/PEKA/"
env_file_path = f"{code_root}/.env"
data_root = code_root + "/DATA/"
ckpt_folder = f"{root}/Pretrained/"
output_dir = f"{root}/OUTPUT/"
all_configs_path = f"{code_root}/hydra_zen/Configs/"
print(f"ğŸ˜ code root folder for project: {code_root}")
print(f"ğŸ’¾ data root folder for project: {data_root}")

print(f"ğŸ¤— ckpt folder for project: {ckpt_folder}")
print(f"ğŸ¯ output folder for project: {output_dir}")
print(f"ğŸ“ƒğŸ“ƒ configs root folder for project: {all_configs_path}")

sys.path.append(root)
sys.path.append(f'/{code_root}/')
sys.path.append(f'/{code_root}/peka/External_models/')
sys.path.append(f'/{code_root}/peka/External_models/HEST/src/')

import peka
import dotenv
dotenv.load_dotenv(env_file_path)
WANDB_API_KEY = os.getenv("WANDB_API_KEY")

HF_TOKEN = os.getenv("HF_TOKEN")

from huggingface_hub import login
# Login to the Hugging Face hub, using your user access token that can be found here:
# https://huggingface.co/settings/tokens.
login(token=HF_TOKEN)


# specify the dataset name and path
tissue_type = "breast" # or "other_cancer"
database_name = "breast_visium_26k" # breast_xenium_100k etc.
database_path = f"{data_root}/{tissue_type}/"
model_name = "scFoundation"

#   load configs 
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.optim as optim
from hydra_zen import builds, instantiate, make_config, zen
from hydra_zen import load_from_yaml

print("ğŸš€ Loading Zen Configs for default settings...")
dataset_config_loc = f"{all_configs_path}/Datasets/{database_name}_{model_name}.yaml"
dataset_config_zen = load_from_yaml(dataset_config_loc)

model_config_loc = f"{all_configs_path}/Models/H-optimus-0_LoRA_MLP.yaml"
#model_config_loc = f"{all_configs_path}/Models/H-optimus-0_LoRA_Transformer.yaml"
model_config_zen = load_from_yaml(model_config_loc)

optimizers_config_loc = f"{all_configs_path}/Optimizers/default.yaml"
optimizers_config_zen = load_from_yaml(optimizers_config_loc)

trainer_config_loc = f"{all_configs_path}/Trainers/default.yaml"
trainer_config_zen = load_from_yaml(trainer_config_loc)

print("ğŸš€ Instantiating Trainer...")
# instantiate trainer
trainer_instance = instantiate(trainer_config_zen,
                                # Basic configurations
        
        entity="shipan_work",
        exp_name="PEKA_scMulan",
        task_type="regression",

        class_nb=1,  # å°†ç±»å‹å£°æ˜ä¸º Optional[int]

        # æ¨¡å‹
        model_name="H-optimus-0_LoRA_Transformer",
        ckpt_folder=ckpt_folder,

        # Model and training components
        # æœ€å¤§ epoch æ•°
        max_epochs=20,
        # è®­ç»ƒè¾“å‡ºç›®å½•
        trainer_output_dir=output_dir,
        additional_pl_paras={}, 
        # æ—¥å¿—
        # wandb å¯†é’¥
        wandb_api_key=WANDB_API_KEY,
                               )

# instantiate dataset, model, and optimizer
print("ğŸš€ Instantiating dataset, model, and optimizer...")
train_loader,val_loader,target_dim = instantiate(dataset_config_zen, data_root=database_path)
model = instantiate(model_config_zen, target_dim=target_dim)
optimizer_instance_list, scheduler_instance_list, metrics_factory, loss_instance = instantiate(optimizers_config_zen)
# instantiate model
from peka.Trainer.pl_basic import pl_basic
pl_model = pl_basic(
    model_instance=model,
    loss_instance=loss_instance(),
    optimizer_instance_list=optimizer_instance_list,
    metrics_factory=metrics_factory,
)


trainer_instance.fit(pl_model, train_loader, val_loader)